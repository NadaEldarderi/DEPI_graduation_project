import streamlit as st
import json
import re
import pickle
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
import spacy
import random
import time

# Page config
st.set_page_config(page_title="Chatbot LSTM Ø§Ù„Ù…Ø·ÙˆØ±", layout="wide", page_icon="ğŸ¤–")

# Load English tokenizer, tagger, parser, NER and word vectors
@st.cache_resource
def load_spacy_model():
    try:
        nlp = spacy.load("en_core_web_sm")
    except Exception:
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©..."):
            spacy.cli.download("en_core_web_sm")
            nlp = spacy.load("en_core_web_sm")
    return nlp

nlp = load_spacy_model()

# Paths
MODEL_PATH = "simple_chatbot_code/simple_chatbot_train_model.h5"
TOKENIZER_PATH = "simple_chatbot_code/tokenizer.pickle"
LABEL_ENCODER_PATH = "simple_chatbot_code/label_encoder.pickle"
MAX_LEN_PATH = "simple_chatbot_code/MAX_LEN.pickle"
TAGS_ANSWERS_PATH = "simple_chatbot_code/tags_answers.pickle"

@st.cache_resource
def load_all_resources():
    model = load_model(MODEL_PATH)

    with open(TOKENIZER_PATH, "rb") as handle:
        tokenizer = pickle.load(handle)

    with open(LABEL_ENCODER_PATH, "rb") as handle:
        lbl_encoder = pickle.load(handle)

    with open(MAX_LEN_PATH, "rb") as handle:
        max_len = pickle.load(handle)

    with open(TAGS_ANSWERS_PATH, "rb") as handle:
        tags_answers = pickle.load(handle)

    return model, tokenizer, lbl_encoder, max_len, tags_answers

model, tokenizer, lbl_encoder, max_len, tags_answers = load_all_resources()

def clean_pattern(msg):
    msg = str(msg).lower().strip()
    msg = re.sub(r'[^A-Za-z]', ' ', msg)
    msg = re.sub(r'\s+', ' ', msg)
    tokens = nlp(msg)
    lemma = [token.lemma_ for token in tokens if not token.is_stop and not token.is_punct]
    return " ".join(lemma).strip()

def predict_intent(text, model, tokenizer, lbl_encoder, max_len):
    cleaned_text = clean_pattern(text)
    sequence = tokenizer.texts_to_sequences([cleaned_text])
    padded_sequence = pad_sequences(sequence, padding="post", maxlen=max_len)
    prediction = model.predict(padded_sequence, verbose=0)
    predicted_label_index = np.argmax(prediction, axis=1)
    predicted_tag = lbl_encoder.inverse_transform(predicted_label_index)[0]
    return predicted_tag

# --- UI Enhancements ---
st.markdown("""
<style>
    .stChatMessage {
        border-radius: 10px;
        padding: 10px;
        margin-bottom: 10px;
    }
    .stChatMessage[data-testid="chatAvatarIcon-user"] + div div {
        background-color: #e6f3ff;
    }
    .stChatMessage[data-testid="chatAvatarIcon-assistant"] + div div {
        background-color: #f0f0f0;
    }
    .typing {
        color: #888;
        font-style: italic;
        animation: blink 1s step-start infinite;
    }
    @keyframes blink {
        50% { opacity: 0; }
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ¤– Chatbot LSTM, DEPI CLS")
st.caption("Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø®Ø¯Ù…ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ")

# Sidebar
with st.sidebar:
    st.header("Ø¹Ù† Ø§Ù„Ø´Ø§Øª Ø¨ÙˆØª")
    st.markdown("Ù‡Ø°Ø§ Ø§Ù„Ø´Ø§Øª Ø¨ÙˆØª ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ LSTM Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±Ø§ØªÙƒ.")
    st.markdown("ØªÙ… ØªØ·ÙˆÙŠØ±Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© ÙØ±ÙŠÙ‚ DEPI Team.")
    st.markdown("* Abdallah Samir\n* Youssef Samy\n* Shaaban Mosaad\n* Nada Amr\n* Mohammed Ahmed Badrawy")
    
    st.markdown("---")
    tone = st.radio("Ø§Ø®ØªØ± Ù†ØºÙ…Ø© Ø§Ù„Ø±Ø¯ÙˆØ¯:", ["Ø±Ø³Ù…ÙŠØ©", "ÙˆØ¯ÙŠØ©"], index=0)
    
    if st.button("Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.messages = []

    if st.session_state.get("messages"):
        st.download_button(
            label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©",
            data=json.dumps(st.session_state.messages, ensure_ascii=False, indent=2),
            file_name="chat_history.json",
            mime="application/json"
        )

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display message count
st.info(f"ğŸ“¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {len(st.session_state.messages)}")

# Display chat messages
for message in st.session_state.messages:
    avatar_icon = "ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "ğŸ¤–"
    with st.chat_message(message["role"], avatar=avatar_icon):
        st.markdown(message["content"])

# Handle user input
if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§..."):
    # User message
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Typing animation
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        with st.spinner("ÙŠÙƒØªØ¨..."):
            time.sleep(1.2)  # simulate delay

            predicted_tag = predict_intent(prompt, model, tokenizer, lbl_encoder, max_len)

            default_response = "Ø¢Ø³ÙØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø°Ù„Ùƒ ØªÙ…Ø§Ù…Ù‹Ø§. Ø­Ø§ÙˆÙ„ ØªØ´Ø±Ø­ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªÙ„ÙØ©."
            response = default_response

            if predicted_tag in tags_answers:
                response = random.choice(tags_answers[predicted_tag])
                if tone == "ÙˆØ¯ÙŠØ©":
                    response = f"ğŸ˜Š {response}"
                else:
                    response = f"{response}."

            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

